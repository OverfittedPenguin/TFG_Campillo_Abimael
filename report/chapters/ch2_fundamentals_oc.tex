\chapter{Fundamentals of Optimal Control and its Implementation}
This chapter includes the mathematical formualtion of the \gls{ocp}, which is the base of the \gls{oct}. It also includes a careful explanation about direct and indirect methods, which are two different approaches to solve optimal control problems, highlighting the \gls{dcm} and \gls{pmp}. Eventually, the \gls{ipopt} method is presented as the used approach, including the mathematical formulation of the method and its implementation from CasADi's Python library.

\section{Introduction to Optimal Control Theory}
The \gls{oct} is a control field related branch where an objective function has to be optimised -most of the times, it has to be minimised- in order to find the control trajectory for a determined dynamical system. This theory is historically realted with calculus of variations, where optimal points -either maxima or minima- are found using little variations in functions or functionals \cite{Courant1953}. This field of study, proposed by Isaac Newton, was developed as a solving approach for the brachistochrone problem, posed by Bernoulli in 1696. The brachistochrone curve is defined as the fastest descent path -most optimal path for minimising time- between two points A and B under a uniform gravitational field. Counterintuitively, it was found that the most optimal path was not but the cycloidal ramp, as can be seen in the image below.

In essence, the \gls{oct} responds the need to solve continuous time optimisation problems, as the one presented. Thus, the \gls{ocp} can be understood as a n-dimensional extension of the \gls{nlp} problem. The NLP problem can be defined as the minimisation of a given function subject to a different set of equations or inequations that act as restrictions -also called constraints- and simple bounds \cite{Kirk2004}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{images/brachistochrone.png}
    \caption{The brachistochrone problem. The ball rolls down the fastest on a cycloidal ramp. Points of the same shade correspond to the same moment in time. Extracted directly from \cite{Blasjo2021}.}
    \label{fig:Brachistochrone}
\end{figure}

\begin{equation}
    \begin{aligned}
        \max_{x\in \mathbb{R}^{n}} \ \  & f(x)\\
        \text{s.t.} \ \  & g_{j}(x)\leq b_{j} \ \text{for each} \ j=1,\dotsc ,m\\
        & x =(x_{1} ,\dotsc ,x_{n} )\\
        & g =(g_{1} ,\dotsc ,g_{m} ).
    \end{aligned}
    \label{eq:NLP}
\end{equation}

The \gls{nlp} problem is limited to a finite number of state variables and restrictions, since it has a discretised treatment. At this point, it has to be mentioned that any \gls{nlp} problem goes through three different phases, which are applicable to the \gls{ocp} or any optimisation problem. The first phase is modelling, which is, in fact, a mathematical description of a dynamic system through simple equations that allow future state predictions of a system thanks to a given set of control variables. While state variables $y_i(t)$ are the variables that describe system conditions at a given time, control variables $u_i(t)$ are the variables that can be manipulated through time because they have an influence on the system. In other words, state variables are consequence of the control variables -p.e for a flight trajectory, the position and velocity of the aircraft would be the state variables for a set of different control variables as could be the \gls{aoa} or the \gls{tps}-. Secondly, the restrictions have to be applied to the system in the form of equality or inequality constraints to describe its physical or operational limitations. In addition, the admissible trajectory and admissible control are described, what means that state and control variables satisfy the restrictions within all time domain, respectively -later will be referred to as simple bounds-. Third and last phase is optimisation, which means the maximisation or minimisation of the cost function or functional associated with the system while complying with all the constraints and bounds set for a determined optimisation criteria. For example, in the brachistochrone problem introduced before, there are several feasible paths between points A and B, but only the cycloidal path complies with the minimum time optimisation criteria \cite{Bernoulli1696}.

It has to be pinpointed that the \gls{ocp} is a n-dimensional interpretation of the \gls{nlp} because state and control variables are time-continuous functions and, therefore, the problem's objective is to find the $n_k$-dimensional vector of state functions $y_k(t)$ and control functions $u_k(t)$ that optimise a determined functional $F(y,u)$.

\subsection{Mathematical formulation}
The \gls{ocp} is defined as follows.

\begin{equation}
    \begin{aligned}
        \min_{y,u} \ \  & J(y,u) \coloneqq \Phi[y(t_f),t_f] + \int _{t_0}^{t_f} L[ y(t),u(t),t] \ dt\\
        \text{s.t.} \ \  & \dot{y} =f[y(t),u(t),t]\\
        & g[y(t),u(t),t] \leq 0\\
        & \varphi [y(t_0 ),u(t_0),t_0] =0\\
        & y_{lb} \leq y(t)\leq y_{ub}\\
        & u_{lb} \leq u(t)\leq u_{ub}\\
        & t\in [t_0 ,t_f]
    \end{aligned}
    \label{eq:OCP}
\end{equation}

The previous description is the formal description for the \gls{ocp}, since it contains a cost functional $J(y,u)$ that has to be minimised subject to a set of constraints and simple bounds \cite{Betts2010}\cite{IllinoisRobotics}. Specifically, the description above has the following terms:

\begin{itemize}
    \item \underline{State variables $y(t)$:} As explained before, state variables are time-continuous functions that describe the system condition and allow to predict its future coindition. This state variables are limited by lower and upper bounds, $y_{lb}$ and $y_{ub}$, respectively. Those bounds are the physical or operational limits that each state variable has; for example, on the brachistochrone problem, the horizontal position of the ball -when movement is restricted to the vertical plane- cannot be lower than the point A horizontal coordinate and bigger than the point B horizontal coordinate -those are the simple bounds for horizontal position state variable in that case-.
    \item \underline{Control variables $u(t)$:} Control variables are time-continuous functions that describe the changes on the system to achieve a determined state or condition through time. Similarly, they are simple bounded, in this case by lower bound $u_{lb}$ and upper bound $u_{ub}$; for example, for the flight trajectory previously described, control variable \gls{aoa} could be lower bounded by a null value and upper bounded by the stall angle, $\alpha_{stall}$ -those are the simple bounds for the \gls{aoa} control variable in that case-.
    \item \underline{Time domain $t$:} Time domain is the closed interval of time, defined on real numbers, between initial time $t_0$ -where initial state condition is applied- and the time horizon $t_f$ -that can be fixed or free-. On \gls{ocp}, time horizon can be set to a known value if its a problem's parameter or can be free if it is set a decision variable -also called control variable- of the problem through end time constraints or inside the cost functional.
    \item \underline{Dynamic constraints $\dot{y}$:} Dynamic equations are a set of \gls{ode}s equality constraints that describe the physics of the problem. This set of constraints are always active, which means that they apply for all time domain. 
    \item \underline{Path constraints $g[y(t),u(t),t]$:} Path contsraints are another set of expressions that apply for a given time domain and that could refer to any type of extra restrcitions to the problem. They could be equalities or inequalities that could both restrict state or controls in a determined way.
    \item \underline{Initial state conditions $\varphi[y(t_0),u(t_0),t_0]$:} The initial state constraints are a set of equalities that restrict the initial value of states variables to a known value -the initial condition of the system is known-. They can also include restrictions for the initial value of control variables; nevertheless, it is not a common practice because the problem could be overrestricted and the solver may be unable to found a feasible solution.
    \item \underline{Cost functional $J(y,u)$:} The cost functional is the cost objective function that has to be minimised to found the optimal solution. This cost is defined by two different terms that contribute differently to the whole objective: terminal and running costs. If both contributions are present, then the cost objective is defined as in its Bolza form. As clarified in \cite{BrysonHo1975}:
    \begin{itemize}
        \item Terminal cost is the contribution that only penalises the final value of state variables. Is represented by $\Phi[y(t_f),u(t_f),t_f]$ and, if cost objective $J(y,u) = \Phi[y(t_f),t_f]$, is it said that the whole cost functional is in its Mayer form.
        \item Running cost is the contribution that penalises each state and control variable trough time and, therefore, accumulates the whole penalisation. Is represented by $\int _{t_0}^{t_f} L[ y(t),u(t),t] \ dt$ and, if cost objective $J(y,u) = \int _{t_0}^{t_f} L[ y(t),u(t),t] \ dt$, is it said that the whole cost functional is in its Lagrange form.
    \end{itemize}
\end{itemize}

\section{Direct and Indirect Methods}
Once the basic problem has been set, is has to be noticed that although the variables are time-continuous, they have to be discretised in order to solve the problem. This discretisation is necessary because the problem is not affordable analytically and it has to be treated numerically. This implicates to discretise the time domain, converting the infinite-dimensional treatment inherited from the running cost into a sum of different time-instants. There are two different types of approaches, as stated by John T. Betts on \cite{Betts2010}:
\begin{itemize}
\item {Direct methods, which are based in the evaluation of the images from a function and the comparison between different values at different instants to find the derivative -p.e Direct collocation method-.}
\item {Indirect methods, which are based on direct evaluation of the derivative and its proximity to zero value. In this case, there is need to explicit derivation of the equations and is a method subjected to initial conditions -p.e Pontryagin's maximum principle-.}
\end{itemize}

\subsection{Direct collocation method as direct method}
Direct collocation methods are a direct method in which the functions are discretised in a set of predefined points called collocation points. These collocation points are part of a collocation grid that is set along the domain in both state and control variables. It has to be pinpointed that between collocation methods two different types can be distinguished: low-order collocation methods, applied to solve \gls{ode}s, and pseudospectral methods, applied to solve \gls{pde}s. Since this bachelor thesis adresses the flight mechanics of an aircraft for low-flight manoeuvres, which dynamic equations consists on different \gls{ode}s, the wording will focus on low-order collocation methods (\gls{dcm} from now on) \cite{Becerra2012}.

Essentially, in the \gls{dcm} the state and the control variables are both discretised in different collocation points. This collocation points are a set of N time-instants that form the collocation grid (in an equivalent way, the reader can think about the collocation grid as the element grid usually made for finite element purposes). On this points, the dynamic of the problem will be evaluated, or forced, to solve the equations and, therefore, the whole problem. Thus, the \gls{dcm} for the control variables is defined as follows, extracted from \cite{Bohme2017}.

\begin{equation}
    \begin{aligned}
        u(t) &= 
        \begin{cases} 
            U_{k}^{u}(u_{k}, u_{k+1}, t) & \forall t \in [t_{k}, t_{k+1}] \\
            U_{N-1}^{u}(u_{N-1}, u_{N}, t) & \forall t \in [t_{N-1}, t_{N}] 
        \end{cases}
    \end{aligned}
    \label{eq:DCM_Control_Discretisation}
\end{equation}

\begin{equation}
    \bar{u} = [u_{0}, u_{1}, u_{2}, \dots, u_{N}]^{T} \in \mathbb{R}^{(N+1) \cdot n_u}
    \label{eq:DCM_Control_Vector}
\end{equation}

The previous expressions \eqref{eq:DCM_Control_Discretisation} and \eqref{eq:DCM_Control_Vector} are the time-domain discretisation in N subintervals with specific nodes $t_i$ with $i \in [0,N]$, what allows to obatin a decision vector for control. In other words, it is an interpolation that avoids sudden discontinuities that can generate inestabilities on the integrator or solver, allowing a more precise capture of the different variations. In an equivalent way, the discretisation for state variables can be obtained as:

\begin{equation}
\overline{y} \ =\ [ y_{0} ,y_{1} ,y_{2} ,\dotsc,y_{N}]^{T} \in \mathbb{R}^{(N+1) \cdot n_y}
\end{equation}


For a better comprehension, the mathematical formulation behind discretisation can be simplified to the following scheme, which exemplifies the conversion of a time-continuous function to its discretised form.

\begin{equation*}
    \begin{aligned}
        u(t)\longrightarrow u_{i} =u(t_{i} )\ \ \ \text{for each} \ t_{i} \ \text{with} \ i\in [0,\dotsc,N]\\
        y(t)\longrightarrow y_{i} =y(t_{i} )\ \ \ \text{for each} \ t_{i} \ \text{with} \ i\in [0,\dotsc,N]
    \end{aligned}
\end{equation*}

At this point, a trapezoidal scheme is proposed to be used on the numerical approach since it takes into account not only the present time-instant but the following time-instant for the function evaulation and its integration. This numerical integration scheme is preferred above others such Runge-Kutta or Hermite-Simpson because of its simplicity, its easy applicability and its precision, which is enough for the scope of this project. The trapezoidal integration scheme for the running cost, adapted from \cite{Becerra2012} and \cite{Jackson2018}, can be state as:

\begin{equation}
    \begin{aligned}
        & \int _{t_{0}}^{t_f} L(y,u,t)\ dt\approx \sum _{k=0}^{N-1}\frac{1}{2} h_{k} \cdotp ( L_{k} +L_{k+1})\\
        where & \\
        & h_{k} \coloneqq t_{k+1} -t_{k}\\
        & L_{k} \coloneqq L[ y(t_{k} ),u(t_{k} ),t_{k}]\\
        & L_{k+1} \coloneqq L[ y(t_{k+1} ),u(t_{k+1} ),t_{k+1}]\\
        & t_{k} \ \text{for each} \ k\in [0,N-1]
    \end{aligned}
    \label{eq:DCM_Running_Cost}
\end{equation}

Furthermore, the trapezoidal rule has to be applied to the constraints of the problem, now called collocation constraints. For the simple bounds, path cosntraints and initial state constraints, the application of the discretisation scheme is easy to reach because the equations have to be evaluated at the given colocation points directly, without need of trapezoidal rule. About the dynamic equations, which are the only ones that have to be integrated by applying the fundamental theorem of calculus, the following development can be obtained, extracted from \cite{Jackson2018}.

\begin{equation*}
    \begin{aligned}
        & \dot{y} =f(y,u,t)\\
        & \int _{t_{k}}^{t_{k+1}}\dot{y} \ dt=\int _{t_{k}}^{t_{k+1}} f(y,u,t)\ dt
    \end{aligned}
\end{equation*}

Resulting in:

\begin{equation}
    \begin{aligned}
        & y_{k+1} - y_{k} =\frac{1}{2} h_{k} \cdotp (f_{k} +f_{k+1} )\\
        where & \\
        & f_{k} =f(y_{k} ,u_{k} ,t_{k} )\text{ for each } k\in [0,N]
    \end{aligned}
    \label{eq:DCM_ODEs}
\end{equation}

Eventually, the terminal cost can be added taking into account its contribution on final time-instant only. Thus, the optimal control problem can be written in its discretised form in a way such that could be solved applying a \gls{dcm}.

\begin{equation}
    \begin{aligned}
        \min_{y,u} \ \  & J(y,u) \coloneqq \Phi[ y(t_{N} ),u(t_{N} ),t_{N}] +\sum _{k=0}^{N-1}\frac{1}{2} h_{k} \cdotp ( L_{k} +L_{k+1})\\
        \text{s.t.} \ \  & y_{k+1} =y_{k} + \frac{1}{2} h_{k} \cdotp (f_{k} +f_{k+1} )\\
        & g[y(t_k),u(t_k),t_k] \leq 0 \text{ for } \forall k\\
        & \varphi [y(t_0 ),u(t_0),t_0] =0\\
        & y_{lb} \leq y(t_k)\leq y_{ub}\\
        & u_{lb} \leq u(t_k)\leq u_{ub}\\
        & t_k \text{ for each } k \in [0,N]
    \end{aligned}
    \label{eq:DCM_OCP}
\end{equation}

\subsection{Pontryagin's maximum principle as indirect method}
As said before, the indirect methods search a solution for the optimality conditions directly. In this context, they doesn't need to discretise the control variables but they will need an approximation; thus, they are subject to the initial conditions. Moreover, since the indirect methods are based on the calculus of variations, they also need to comply a set of necessary conditions called transversality conditions. These remarks about conditions are important and are stated before presenting the \gls{pmp}.

The optimality conditions are consequence of the application of the Euler-Lagrange equation, which can be found on \cite{UCLCalcVar}. They mean to be the necessary but not sufficient conditions for the resolution of the optimisation problem; in other words, they are the proof that the solution found is actually the optimal one. They are careless of meaning until they are directly found for the problem. They are also called \gls{kkt} conditions. Essentially, they are a set of equations related to the gradient or derivative of a functional respect to other parameters and variables that have to be null in order to verify optimality. A general optimisation problem and its Lagrangian can be defined as follows.

\begin{equation}
    \begin{aligned}
        min_{x\in \mathbb{R}^{n}} \ \  & f\ (x)\\
        \text{s.t.} \ \  & g_{i} \ (x)\leq 0\ \text{for each} \ i=1,\dotsc ,m\\
        & h_{j} \ ( x) =0\ \text{for each} \ j=1,\dotsc ,p\\
    \end{aligned}
    \label{eq:OP}
\end{equation}

\begin{equation}
    \mathcal{L}(x,z ,\mu )\coloneqq f(x)+\sum _{i=0}^{m} z _{i} g_{i} (x)++\sum _{j=0}^{p} \mu _{j} h_{j} (x)
    \label{eq:OP_Lagrangian}
\end{equation}

The \gls{kkt} conditions are stationarity, complementary slackness, primal feasability and dual feasability. Its meaning and mathematical formulation can be seen below as an adaptation from \cite{GordonTibshirani}.

\begin{itemize}
\item Stationarity. Referred to the balanced effect of the gradient of the active constraints and the gradient of the objective functions. In other words, it describes how each constraint influences the cost function itself, ensuring that at the optimum no further improvement is possible without violating constraints.
\end{itemize}

\begin{equation}
    \nabla _{x}\mathcal{L} (x,z ,\mu )\coloneqq\nabla f(x)+\sum _{i=0}^{m} z_{i} \nabla g_{i} (x)+\sum _{j=0}^{p} \mu _{j} \nabla h_ {j} (x)=0
    \label{eq:KKT_Stationarity}
\end{equation}

\begin{itemize}
\item Complementary slackness. The multiplier reflects how much the constraint influences the optimal solution. For each inequality constraint, either the constraint is active and its Lagrange multiplier can be positive, or the constraint is inactive and its multiplier is zero.
\end{itemize}

\begin{equation}
    z_{i} g_{i}(x) =0
    \label{eq:KKT_Comp_Slackness}
\end{equation}

\begin{itemize}
\item Primal feasability. Ensures that the found solution satisfies the original constraints of the problem, which means that the inequality conditions must be held and the equality conditions must be satisfied totally.
\end{itemize}

\begin{equation}
    \begin{aligned}
        & g_{i} (x)\leq 0\\
        & h_{j} (x)=0
    \end{aligned}
    \label{eq:KKT_Primal_Inf}
\end{equation}

\begin{itemize}
\item Dual feasability. Related to inequality constraints multipliers, they verify they are non-negative values and describe how would the cost and the solution change if these constraints are relaxed or inactivated.
\end{itemize}

\begin{equation}
    z _{i} \geq 0
    \label{eq:KKT_Dual_Inf}
\end{equation}

On its part, the transversality conditions are the boundary conditions for the co-state variables $\lambda$. They are a necessary condition for optimisation in state free-end problems, which means that there's no end known for the trajectory. These transversality conditions can be defined as the existence of a n-vectorial function of the co-state variables $\lambda (t)$ and a m-vectorial function $\nu (t)$ such that the Hamiltonian function $ H(\lambda,\nu,t)$, see equation \eqref{eq:Hamiltonian}, can be defined like in a boundary value problem in his canonical form with \gls{ode}s in all time domain $t_{0} \leq t\leq t_f$ \cite{Stryk1992}\cite{Leonard1992}.

\begin{equation}
    H\ (\lambda ,\nu ,t)\coloneqq\lambda ^{T} f+\nu ^{T} g
    \label{eq:Hamiltonian}
\end{equation}

Once the Hamiltonian is defined, both transversality conditions can be found as the partial derivative of the Hamiltonian respect to the state variables and, the evaluation of the co-state at the end of time domain, respectively. Formulation has been adapted from \cite{Stryk1992} and \cite{Chachuat2009}.

\begin{equation}
    \dot{\lambda } \ =\ -\frac{\partial H}{\partial y} =-\lambda ^{T}\frac{\partial f}{\partial y} -\nu ^{T}\frac{\partial g}{\partial y}
    \label{eq:Transversality_1}
\end{equation}

\begin{equation}
    \lambda (t_f) =\ \frac{\partial \Phi }{\partial y(t_f)} -\nu ^{T}\frac{\partial \varphi }{\partial y(t_f)}
    \label{eq:Transversality_2}
\end{equation}

Eventually, the \gls{pmp} is a fundamental result of optimal control theory, since it stablishes the necessary conditions to find the optimal trajectory for control variables. In other words, this principle gives the strategy that has to be followed on the control variables, the ones that can be modified through time on the whole problem, to minimise the cost functional while the system dynamic and its constraints are satisfied. From \cite{ColumbiaPMP}, the \gls{pmp} is defined as the partial derivative of the Hamiltonian function respect to the control variables for a problem where $y(0) = y_{0} \in \mathcal{S}$ and $u(t)$, a contol trajectory, has a state trajectory $y(t)$  associated for a given dynamic system. Thus, a co-state trajectory $\lambda (t)$ can be defined as follows.

\begin{equation}
    \begin{aligned}
        & \dot{\lambda } (t)=-\frac{\partial H}{\partial y}\\
        & \lambda (t_f)=\frac{\partial \Phi }{\partial y(t_f)}\\
        where & \\
        & H(y,u,\lambda ,t)\coloneqq L(y,u,t)+\lambda ^{T} f(y,u,t)\\
        & \dot{y} (t)=f(y,u,t)
    \end{aligned}
    \label{eq:PMP_Problem}
\end{equation}

Therefore, the \gls{pmp} can be expressed as the minimisation of the Hamiltonian with respect to the control variables, as indicated in equation \eqref{eq:PMP}. This formulation arises because the original statement of \gls{pmp} is given as a maximisation problem, but it can be equivalently rewritten in minimisation form by changing the sign of the Hamiltonian \cite{BrysonHo1975_Blaisdell} -also called Pontryagin's Minimum Principle-. In essence, the principle reformulates the original optimal control problem into an equivalent system involving the state and an adjoint -or costate- system. The optimal control trajectory is then obtained by ensuring that, at each instant of time, the Hamiltonian is minimised with respect to the admissible controls.

\begin{equation}
    \begin{aligned}
        u(t)\coloneqq\arg\min_{u\in U} \ H\ (y,u,\lambda ,t)
    \end{aligned}
    \label{eq:PMP}
\end{equation}

\section{Introduction to IPOPT}
The \gls{ipopt} is a robust interior-point line-search algorithm designed to solve large-scale \gls{nlp} problems. This method is particularly well-suited for high-dimensional trajectory optimisation, such as the low-altitude flight manoeuvres required during water-discharge phases in firefighting missions -the primary focus of this thesis-.

\gls{ipopt} identifies local optimal solutions by employing a primal-dual barrier approach to handle inequality constraints and a filter-based line-search method to ensure global convergence. The following subsections detail the mathematical formulation of the \gls{ipopt} algorithm and its practical implementation via the CasADi numerical optimisation framework.

\subsection{Mathematical formulation}
Below there is a general formulation of an optimisation problem followed by its analogue in barrier problem notation. The barrier problem method is a technique used to transform inequalities in constrained optimisation problems into an unconstrained problem, which is easy to solve.

\begin{equation}
    \begin{aligned}
        \min_{x\in \mathbb{R}^{n}} \ \  & f(x)\\
        \text{s.t.} \ \  & c(x) = 0\\
                         & x\geq 0
    \end{aligned}
    \label{eq:Opt_Problem}
\end{equation}

\begin{equation}
    \begin{aligned}
        \min_{x\in \mathbb{R}^{n}} \ \  & \varphi_\mu(x) \coloneqq f(x) - \mu \sum_{i=0}^n ln(x^{(i)})\\
        \text{s.t.} \ \  & c(x) = 0\\
    \end{aligned}
    \label{eq:Barrier_Problem}
\end{equation}

The barrier method transforms the inequality constraints of a problem into a sequence of equality-constrained subproblems that are easier to solve numerically. Once the barrier problem is defined, the IPOPT algorithm computes an approximate solution for a fixed value of the barrier parameter $\mu$. This process is iterative: for each $\mu_j$, the algorithm seeks a local solution to the subproblem. Upon reaching a specific inner tolerance, the barrier parameter is decreased, and the previous solution is used as a 'warm start' for the next subproblem.

\begin{equation}
    \mathcal{L}(x,\lambda,z) \coloneqq f(x) + c(x)^T\lambda - x^Tz
    \label{eq:BP_Lagrangian}
\end{equation}

Following the Lagrangian, the \gls{kkt} conditions can be defined for the barrier problem using diagonal matrices for $x$ variables and $z$ multipliers on complementary slackness definition, which is defined using a tolerance $e$. 

\begin{subequations}
    \begin{align}
        \nabla f(x) + \nabla c(x) \lambda - z &= 0 \\
        c(x) &= 0 \\
        XZe - \mu e &= 0
    \end{align}
    \label{eq:BP_KKT}
\end{subequations}

At this point, optimality error of the barrier problem can be defined using \gls{kkt} conditions. The error is the ratio that would be used to compare with tolerances to determine if local and global optimal points are reached. Full definition of $s_d$ and $s_c$ can be found on \cite{Waechter2004}.

\begin{equation}
    E_\mu(x,\lambda,z)\coloneqq \text{max}\left\{\frac{||\nabla f(x) + \nabla c(x)-z||_\infty}{s_d}, \frac{||XZe-\mu e||_\infty}{s_c}\right\}
    \label{eq:BP_Error}
\end{equation}

The expression above is used on each iteration of the \gls{ipopt} algorithm to compute the optimality error and, therefore, check local convergence before stepping onto the following point. The checks for local and global convergence are the following, respectively. As can be seen, these checks use approximate solution and an approximation to multipliers ($\tilde{x}^*, \tilde{\lambda}^*, \tilde{z}^*$). It also has to be pinpointed that each iteration, the update in barrier parameter is obtained following the expression \eqref{eq:BP_Mu}. All three expresions are computed for given constants $\kappa_\epsilon$, $\kappa_\mu$, $\theta_\mu$ and $\epsilon_{tol}$, as indicated in \cite{Waechter2004}.

\begin{equation}
    E_{\mu, j}(\tilde{x}^*_{j+1}, \tilde{\lambda}^*_{j+1}, \tilde{z}^*_{j+1}) \leq \kappa_\epsilon \mu_j
    \label{eq:BP_Local_Convergence}
\end{equation}

\begin{equation}
    E_0(\tilde{x}^*, \tilde{\lambda}^*, \tilde{z}^*) \leq \epsilon_{tol}
    \label{eq:BP_Global_Convergence}
\end{equation}

\begin{equation}
    \mu_{j+1} = \max \left\{ \frac{\epsilon_{tol}}{10}, \min \left\{ \kappa_{\mu} \mu_j, \mu_j^{\theta_{\mu}} \right\} \right\}
    \label{eq:BP_Mu}
\end{equation}

As said before, the \gls{ipopt} is based on a primal-dual framework that iterates between variables and multipliers by solving a linearised set of the \gls{kkt} conditions via Newton's method. This method allows to find gradients for each variable or multiplier, as stated below.

\begin{equation}
    \begin{bmatrix}
    W_k & A_k^T & -I \\
    A_k & 0 & 0 \\
    Z_k & 0 & X_k
    \end{bmatrix}
    \begin{pmatrix}
    d_x^k \\
    d_{\lambda}^k \\
    d_z^k
    \end{pmatrix} = -
    \begin{pmatrix}
    \nabla f(x_k) + A_k^T \lambda_k - z_k \\
    c(x_k) \\
    X_k Z_k e - \mu_j e
    \end{pmatrix}
    \label{eq:BP_Newton_System}
\end{equation}

From the previous expression, gradients could be computed. Thus, the update on variables and multipliers will be set following the set of equations \eqref{eq:BP_Mu} once the step size parameter $\alpha_k$ is determined by a filter search or filter method. A deep explanation of filther methods used can be found on \cite{Waechter2004} and \cite{Fletcher2002}.

\begin{subequations}
    \begin{align}
        x_{k+1} &\coloneqq x_k + \alpha_k d_x^k \\
        \lambda_{k+1} &\coloneqq \lambda_k + \alpha_k d_{\lambda}^k \\
        z_{k+1} &\coloneqq z_k + \alpha_z^k d_z^k
    \end{align}
    \label{eq:BP_Updated_Steps}
\end{subequations}

Eventually, a summarised version of the general \gls{ipopt} algorithm has been adapted from the original source \cite{Waechter2004}.

\begin{algorithm}
\caption{Interior-Point Filter Line-Search Algorithm (IPOPT)}
\begin{algorithmic}
\STATE \textbf{1. Initialise:} Choose $x_0 > 0$, $\lambda_0, z_0 > 0$, $\mu > 0$, and iteration limit $N$.
\FOR{$k = 0$ \TO $N$}
    \STATE \textbf{2. Compute Errors:} Calculate $E_\mu(x_k, \lambda_k, z_k)$ and $E_0(x_k, \lambda_k, z_k)$.
    
    \IF{$E_0 \leq \epsilon_{tol}$}
        \STATE \textbf{STOP:} Optimal solution found at iteration $k$.
    \ENDIF

    \IF{$E_\mu \leq \kappa_\epsilon \mu$}
        \STATE \textbf{3. Update Barrier:} Decrease $\mu$ and return to Step 2.
    \ENDIF

    \STATE \textbf{4. Solve KKT System:} Compute search directions $d_k$ via expression \eqref{eq:BP_Newton_System}.
    
    \STATE \textbf{5. Filter Line-Search:} Determine $\alpha_k$ via the filter method.
    
    \STATE \textbf{6. Update Iterates:} $x_{k+1} = x_k + \alpha_k d_k^x$, etc.
    
\ENDFOR
\IF{$k = N$}
    \STATE \textbf{Exit:} Maximum iterations reached without convergence.
\ENDIF
\end{algorithmic}
\end{algorithm}

\subsection{Implementation of IPOPT using CasADi}\label{sec:CasADi}
\gls{ipopt} is one of the integrated solvers provided by CasADi inside the \textit{ca.nlpsol()} block. This solver blocks needs from a very specific definition of the problem initial guess, its variables, constraints and simple bounds using symbolic notation. This specific definition has to end in the creation of a \textit{nlp\{\}} dictionary that contains all the discretised symbolic variables that form the \gls{nlp} problem. In other words, from the \gls{ocp}, a discretisation has to be applied to reach a fesaible \gls{nlp} definition with initial guess, constraints and simple bounds defined for each variable at each collocation point -all those defined using symbolic notation-. In addition, this dictionary has to include the definition of the cost function as the sum of all variables, when needed.

Since the needs explained above, suppose a simple problem directly extracted from \gls{nlp} CasADi documentation section \cite{Andersson2018}. In this problem, there are three different variables, an objective function and an equality cosntraint.

\begin{equation}
    \begin{aligned}
        \min_{x, y, z} \quad & f(x,z) \coloneqq x^2 + 100z^2 \\
        \text{s.t.} \quad & z + (1 - x)^2 - y = 0
    \end{aligned}
    \label{eq:CasADi_Example}
\end{equation}

The guidelines for the creation of \gls{nlp} problem dictionary are:

\begin{itemize}
    \item \underline{State and control vector $w$}. The state and control vector has to contain all the states and controls in symbolic notation for each collocation point, from the first to the last, following an order. On the example set, since there is a unique collocation point and there are three variables, the definition will be $w = (x,y,z)$ -in the case there were more than one collocation point, $w=(x_0,y_0,z_0, x_1,y_1,z_1 \dotsc x_N,y_n,z_N)$-.
    \item \underline{Cost function $f(x,z)$:} The cost function has to include the penalties defined for each variable when needed. In the example, there is a unique cost defined. Nevertheless, if it was defined for more than one collocation point it would be translated into \textit{ca.nlpsol()} needs like stated in \eqref{eq:CasADi_NLP_Cost}.

    \begin{equation}
        F(x,z) \coloneqq x_0^2 + 100z_0^2 + x_1^2 + 100z_1^2 + \dotsc + x_N^2 + 100z_N^2
        \label{eq:CasADi_NLP_Cost}
    \end{equation}

    \item \underline{Constraint $g(x,y,z)$:} The constraint vector has to the different constraints by order of appearance and, following the remarks on cost function, they would have to be defined for each collocation point variables when needed.
\end{itemize}

From the remarks above, the dictionary for \textit{ca.nlpsol()} can be defined. Its definition is inserted inside an end-to-end example of programming implementation. The bases set are the ones that would be later used for the thesis problem definition and implementation.

\begin{lstlisting}[language=Python, caption={Example definition using CasADi \textit{ca.nlpsol()} block and IPOPT solver. Extracted from \cite{Andersson2018}.}, label={lst:CasADi_Example}]
import casadi as ca
x = ca.SX.sym('x'); y = ca.SX.sym('y'); z = ca.SX.sym('z')
nlp = {'x':vertcat(x,y,z), 'f':x**2+100*z**2, 'g':z+(1-x)**2-y}
S = ca.nlpsol('S', 'ipopt', nlp)
\end{lstlisting}

Another important remark lays into the formulation of numeric initial guess, which has to be initialised in each variable defined on state and control vector $w$. In other words, solver would need an analogue initial state and control vector $w_0$. Furthermore, constraints are defined following the order provided in discretised \gls{ocp} definition, see \eqref{eq:DCM_OCP}, which is: dynamic constraints, path constraints and initial state constraints. As state before, those remarks apply for all variables and would have to be defined following the same order that has been defined in $w$. CasADi does not accept matrices easily, which means that constraints vector $g$ will need $g_{lb}$ and $g_{ub}$ provided as a vector -noted as \textit{lbg} and \textit{ubg} through all thesis and provided code-. That last remark also applies to simple bounds $x_{lb}$ and $x_{ub}$, noted as \textit{lbx} and \textit{ubx}, respectively.