\chapter{Fundamentals of Optimal Control and its Implementation}
This chapter includes the mathematical formualtion of the \gls{ocp}, which is the base of the \gls{oct}. It also includes a careful explanation about direct and indirect methods, which are two different approaches to solve optimal control problems, highlighting the \gls{dcm} and \gls{pmp}. Eventually, the \gls{ipopt} is presented as the used approach, including the mathematical formulation of the method and its implementation from CasADi's Python library.

\section{Introduction to Optimal Control Theory}
The \gls{oct} is a control field related branch where an objective function has to be optimised -most of the times, it has to be minimised- in order to find the control trajectory for a determined dynamical system. This theory is historically realted with calculus of variations, where optimal points -either maxima or minima- are found using little variations in functions or functionals (\textcolor{red}{CITA 1}). This field of study, proposed by Isaac Newton, was developed as a solving approach for the brachistochrone problem, posed by Bernoulli in 1696. The brachistochrone curve is defined as the fastest descent path -most optimal path for minimising time- between two points A and B under a uniform gravitational field. Counterintuitively, it was found that the most optimal path was not but the cycloidal ramp, as can be seen in the image below, where the ball has to roll under gravity only at the different paths, and the same grade shaded figures correspond to same time instants.

In essence, the \gls{oct} responds the need to solve continuous time optimisation problems, as once presented. Thus, the \gls{ocp} can be understood as a n-dimensional extension of the \gls{nlp} problem. The NLP problem can be defined as the minimisation of a given function subject to a different set of equations or inequations that act as restrictions -also called constraints- and simple bounds (\textcolor{red}{CITA 3}).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{images/brachistochrone.png}
    \caption{The brachistochrone problem. The ball rolls down the fastest on a cycloidal ramp. Points of the same shade correspond to the same moment in time. Extracted directly from \textcolor{red}{CITA 2}}
    \label{fig:Brachistochrone}
\end{figure}

\begin{equation}
    \begin{aligned}
        \max_{x\in \mathbb{R}^{n}} \ \  & f\ (x)\\
        \text{s.t.} \ \  & g_{j} \ (x)\leq b_{j} \ \text{for each} \ j=1,\dotsc ,m\\
        & x=(x_{1} ,\dotsc ,x_{n} )\\
        & g=(g_{1} ,\dotsc ,g_{m} ).
    \end{aligned}
    \label{eq: NLP}
\end{equation}

The \gls{nlp} problem is limited to a finite number of state variables and restrictions, since it has a discretised treatment. At this point, it has to be mentioned that any \gls{nlp} problem goes through three different phases, which are applicable to the \gls{ocp} or any optimisation problem. The first phase is modelling, which is, in fact, a mathematical description of a dynamic system through simple equations that allow future state predictions of a system thanks to a given set of control variables. While state variables $y_i(t)$ are the variables that describe system conditions at a given time, control variables $u_i(t)$ are the variables that can be manipulated through time because they have an influence on the system. In other words, state variables are consequence of the control variables -p.e for a flight trajectory, the position and velocity of the aircraft would be the state variables for a set of different control variables as could be the \gls{aoa} or the \gls{tps}-. Secondly, the restrictions have to be applied to the system in the form of equality or inequality constraints to describe its physical or operational limitations. In addition, the admissible trajectory and admissible control are described; that means that state and control variables satisfy the restrictions within all time domain, respectively -later will be referred to as simple bounds-. Third and last phase is optimisation, which means the maximisation or minimisation of the cost function or functional associated with the system while complying with all the constraints and bounds set for a determined optimisation criteria. For example, in the brachistochrone problem introduced before, there are several feasible paths between points A and B, but only the cycloidal path complies with the minimum time optimisation criteria (\textcolor{red}{CITA 4}).

It has to be pinpointed that the \gls{ocp} is a n-dimensional interpretation of the \gls{nlp} because state and control variables are time-continuous functions and, therefore, the problem's objective is to find the $n_k$-dimensional vector of state functions $y_k(t)$ and control functions $u_k(t)$ that optimise a determined functional $F(y,u)$.

\subsection{Mathematical formulation}
The \gls{ocp} is defined as follows.

\begin{equation}
    \begin{aligned}
        \min_{y,u} \ \  & J(y,u) := \Phi[y(t_f),t_f] + \int _{t_0}^{t_f} L[ y(t),u(t),t] \ dt\\
        \text{s.t.} \ \  & \dot{y} =f[y(t),u(t),t]\\
        & g[y(t),u(t),t] \leq 0\\
        & \varphi [y(t_0 ),u(t_0),t_0] =0\\
        & y_{lb} \leq y(t)\leq y_{ub}\\
        & u_{lb} \leq u(t)\leq u_{ub}\\
        & t\in [t_0 ,t_f]
    \end{aligned}
    \label{eq: OCP}
\end{equation}

The previous description is the formal description for the \gls{ocp}, since it contains a cost functional $J(y,u)$ that has to be minimised subject to a set of constraints and simple bounds \textcolor{red}{CITAS 5 y 6}. Specifically, the description above has the following terms:

\begin{itemize}
    \item \underline{State variables $y(t)$:} As explained before, state variables are time continuous functions that describe the system condition and allow to predict its future coindition. This state variables are limited by lower and upper bounds, $y_{lb}$ and $y_{ub}$, respectively. Those bounds are the physical or operational limits that each state variable has; for example, on the brachistochrone problem, the horizontal position,of the ball -when movement is restricted to the vertical plane- cannot be lower than the point A horizontal coordinate and bigger than the point B horizontal coordinate -those are the simple bounds for horizontal position state variable in that case-.
    \item \underline{Control variables $u(t)$:} Control variables are time continuous functions that describe the changes on the system to achieve a determined state or condition through time. Similarly, they are simple bounded, in this case by lower bound $u_{lb}$ and upper bound $u_{ub}$; for example, for the flight trajectory previously described, control variable \gls{aoa} could be lower bounded by a null value and upper bounded by the stall angle, $\alpha_{stall}$ -those are the simple bounds for the \gls{aoa} control variable in that case-.
    \item \underline{Time domain $t$:} Time domain is the closed interval of time, defined on real numbers, between initial time $t_0$ -where initial state condition is applied- and the time horizon $t_f$ -that can be fixed or free-. On \gls{ocp}, time horizon can be set to a known value if its a problem's parameter or can be free if it is set a decision variable -also called control variable- of the problem through end time constraints or inside the cost functional.
    \item \underline{Dynamic constraints $\dot{y}$:} Dynamic equations are a set of \gls{ode}s equality constraints that describe the physics of the problem. This set of constraints are always active, which means that they apply for all time domain. 
    \item \underline{Path constraints $g[y(t),u(t),t]$:} Path contsraints are another set of expressions that apply for a given time domain and that could refer to any type of extra restrcitions to the problem. They could be equalities or inequalities that could both restrict state or controls in a determined way.
    \item \underline{Initial state conditions $\varphi[y(t_0),u(t_0),t_0]$:} The initial state constraints are a set of equalities that restrict the initial value of states variables to a known value -the initial condition of the system is known-. They can also include restrictions for the initial value of control variables; nevertheless, it is not a common practice because the problem could be overrestricted and the solver may be unable to found a feasible solution.
    \item \underline{Cost functional $J(y,u)$:} The cost functional is the cost objective function that has to be minimised to found the optimal solution. This cost is defined by two different terms that contribute differently to the whole objective: terminal and running costs If both contributions are present, then the cost objective is defined as in its Bolza form. As clarified in \textcolor{red}{Bryson, A. E.,\& Ho, Y. C. (1975). Applied Optimal Control: Optimization, Estimation, and Control. Hemisphere Publishing Corp}:
    \begin{itemize}
        \item Terminal cost is the contribution that only penalises the final value of state variables. Is represented by $\Phi[y(t_f),u(t_f),t_f]$ and, if cost objective $J(y,u) = \Phi[y(t_f),t_f]$, is it said that the whole cost functional is in its Mayer form.
        \item Running cost is the contribution that penalises each state and control variable trough time and, therefore, accumulates the whole penalisation. Is represented by $\int _{t_0}^{t_f} L[ y(t),u(t),t] \ dt$ and, if cost objective $J(y,u) = \int _{t_0}^{t_f} L[ y(t),u(t),t] \ dt$, is it said that the whole cost functional is in its Lagrange form.
    \end{itemize}
\end{itemize}

\section{Direct and Indirect Methods}
Once the basic problem has been set, is has to be noticed that although the variables are time-continuous, they have to be discretized in order to solve the problem. This discretization is necessary because the problem is not affordable analytically and it has to be treated numerically. This implicates to discretize the time domain, converting the infinite-dimensional treatment inherited from the running cost into a sum of different time-instants. There are two different types of approaches, as stated by John T. Betts on \textcolor{red}{CITA 5}:
\begin{itemize}
\item {Direct methods, which are based in the evaluation of the images from a function and the comparison between different values at different instants to find the derivative -p.e Direct collocation method-.}
\item {Indirect methods, which are based on direct evaluation of the derivative and its proximity to zero value. In this case, there is need to explicit derivation of the equations and is a method subjected to initial conditions -p.e Pontryagin's maximum principle-.}
\end{itemize}

\subsection{Direct collocation method as direct method}
Direct collocation methods are a direct method in which the functions are discretised in a set of predefined points called collocation points. These collocation points are part of a collocation grid that is set along the domain in both state and control variables. It has to be pinpointed that between collocation methods two different types can be distinguished: low-order collocation methods, applied to solve \gls{ode}s, and pseudospectral methods, applied to solve \gls{pde}s. Since this bachelor thesis adresses the flight mechanics of an aircraft for low-flight manoeuvres, which dynamic equations consists on different \gls{ode}s, the wording will focus on low-order collocation methods (\gls{dcm} from now on) \textcolor{red}{CITA 9}.

Essentially, in the \gls{dcm} the state and the control variables are both discretised in different collocation points. This collocation points are a set of N time-instants that form the collocation grid (in an equivalent way, the reader can think about the collocation grid as the element grid usually made for FEM or CFD). On this points, the dynamic of the problem will be evaluated, or forced, to solve the equations and, therefore, the whole problem. Thus, the \gls{dcm} for the control variables is defined as follows, extracted from \textcolor{red}{CITA 7}.

\begin{equation}
    \begin{aligned}
        u(t) &= 
        \begin{cases} 
            U_{k}^{u}(u_{k}, u_{k+1}, t) & \forall t \in [t_{k}, t_{k+1}] \\
            U_{N-1}^{u}(u_{N-1}, u_{N}, t) & \forall t \in [t_{N-1}, t_{N}] 
        \end{cases}
    \end{aligned}
    \label{eq:DCM_Control_Discretisation}
\end{equation}

\begin{equation}
    \bar{u} = [u_{0}, u_{1}, u_{2}, \dots, u_{N}]^{T} \in \mathbb{R}^{(N+1) \cdot n_u}
    \label{eq:DCM_Control_Vector}
\end{equation}

The previous expressions \eqref{eq:DCM_Control_Discretisation} and \eqref{eq:DCM_Control_Vector} are the time-domain discretisation in N subintervals with specific nodes $t_i$ with $i \in [0,N]$, what allows to obatin a decision vector for control. In other words, it is an interpolation that avoids sudden discontinuities that can generate inestabilities on the integrator or solver, allowing a more precise capture of the different variations. In an equivalent way, the discretisation for state variables can be obtained as:

\begin{equation}
\overline{y} \ =\ [ y_{0} ,y_{1} ,y_{2} ,\dotsc,y_{N}]^{T} \in \mathbb{R}^{(N+1) \cdot n_y}
\end{equation}


For a better comprehension, the mathematical formulation behind discretisation can be simplified to the following scheme, which exemplifies the conversion of a time-continuous function to its discretised form.

\begin{equation*}
    \begin{aligned}
        u(t)\longrightarrow u_{i} =u(t_{i} )\ \ \ \text{for each} \ t_{i} \ \text{with} \ i\in [0,\dotsc,N]\\
        y(t)\longrightarrow y_{i} =y(t_{i} )\ \ \ \text{for each} \ t_{i} \ \text{with} \ i\in [0,\dotsc,N]
    \end{aligned}
\end{equation*}

At this point, a trapezoidal scheme is proposed to be used on the numerical approach since it takes into account not only the present time-instant but the following time-instant for the function evaulation and its integration. This numerical integration scheme is preferred above others such Runge-Kutta or Hermite-Simpson because of its simplicity, its easy applicability and its precision, which is enough for the scope of this project. The trapezoidal integration scheme for the running cost, adapted from \textcolor{red}{CITAS 8 y 9}, can be state as:

\begin{equation}
    \begin{aligned}
        & \int _{t_{0}}^{t_f} L(y,u,t)\ dt\approx \sum _{k=0}^{N-1}\frac{1}{2} h_{k} \cdotp ( L_{k} +L_{k+1})\\
        where & \\
        & h_{k} =t_{k+1} -t_{k}\\
        & L_{k} =L[ y(t_{k} ),u(t_{k} ),t_{k}]\\
        & L_{k+1} =L[ y(t_{k+1} ),u(t_{k+1} ),t_{k+1}]\\
        & t_{k} \ \text{for each} \ k\in [0,N-1]
    \end{aligned}
    \label{eq:DCM_Running_Cost}
\end{equation}

Furthermore, the trapezoidal rule has to be applied to the constraints of the problem, now called collocation constraints. For the simple bounds, path and initial state constraints, the application of the discretisation scheme is easy to reach because the equations have to be evaluated at the given colocation points directly, without need of trapezoidal rule. About the dynamic equations, which are the only ones that have to be integrated by applying the fundamental theorem of calculus, the following development can be obtained, extracted from \textcolor{red}{CITA 8}.

\begin{equation*}
    \begin{aligned}
        & \dot{y} =f(y,u,t)\\
        & \int _{t_{k}}^{t_{k+1}}\dot{y} \ dt=\int _{t_{k}}^{t_{k+1}} f(y,u,t)\ dt
    \end{aligned}
\end{equation*}

Resulting in:

\begin{equation}
    \begin{aligned}
        & y_{k+1} - y_{k} =\frac{1}{2} h_{k} \cdotp (f_{k} +f_{k+1} )\\
        where & \\
        & f_{k} =f(y_{k} ,u_{k} ,t_{k} )\text{ for each } k\in [0,N]
    \end{aligned}
    \label{eq:DCM_ODEs}
\end{equation}

Eventually, the terminal cost can be added taken into account its contribution on final time-instant only. Thus, the optimal control problem can be written in its discretized form in a way such that could be solved applying a \gls{dcm}.

\begin{equation}
    \begin{aligned}
        \min_{y,u} \ \  & J(y,u) := \Phi[ y(t_{N} ),u(t_{N} ),t_{N}] +\sum _{k=0}^{N-1}\frac{1}{2} h_{k} \cdotp ( L_{k} +L_{k+1})\\
        \text{s.t.} \ \  & y_{k+1} =y_{k} + \frac{1}{2} h_{k} \cdotp (f_{k} +f_{k+1} )\\
        & g[y(t_k),u(t_k),t_k] \leq 0 \text{ for } \forall k\\
        & \varphi [y(t_0 ),u(t_0),t_0] =0\\
        & y_{lb} \leq y(t_k)\leq y_{ub}\\
        & u_{lb} \leq u(t_k)\leq u_{ub}\\
        & t_k \text{ for each } k \in [0,N]
    \end{aligned}
    \label{eq:DCM_Terminal_Cost}
\end{equation}

\subsection{Pontryagin's maximum principle as indirect method}
As said before, the indirect methods search a solution for the optimality conditions directly. In this context, they doesn't need to discretise the control variables but they will need an approximation; thus, they are subject to the initial conditions. Moreover, since the indirect methods are based on the calculus of variations, they also need to comply a set of necessary conditions called transversality conditions. These remarks about conditions are important and are stated before presenting the \gls{pmp}.

The optimality conditions are consequence of the application of the Euler-Lagrange equation, which can be found on \textcolor{red}{CITA 14}. They mean to be the necessary but not sufficient conditions for the resolution of the optimiSation problem; in other words, they are thre proof that the solution found is actually the optimal one. They are careless of meaning until they are directly found for the problem. They are also called \gls{kkt} conditions. Essentially, they are a set of equations related to the gradient or derivative of a functional respect other parameters and variables that have to be null in order to verify optimality. A general optimisation problem and its Lagrangian can be defined as follows.

\begin{equation}
    \begin{aligned}
        min_{x\in \mathbb{R}^{n}} \ \  & f\ (x)\\
        \text{s.t.} \ \  & g_{i} \ (x)\leq 0\ \text{for each} \ i=1,\dotsc ,m\\
        & h_{j} \ ( x) =0\ \text{for each} \ j=1,\dotsc ,p\\
    \end{aligned}
    \label{eq:OP}
\end{equation}

\begin{equation}
    \mathcal{L} \ (x,\lambda ,\mu )=f(x)+\sum _{i=0}^{m} \lambda _{i} g_{i} (x)++\sum _{j=0}^{p} \mu _{j} h_{j} (x)
    \label{eq:OP_Lagrangian}
\end{equation}

The \gls{kkt} conditions are stationarity, complementary slackness, primal feasability and dual feasability. Its meaning and mathematical formulation can be seen below as an adaptation from \textcolor{red}{CITA 15}.

\begin{itemize}
\item Stationarity. Referred to the balanced effect of the gradient of the active constraints and the gradient of the objective functions. In other words, it describes how each constraint influences the cost function itself, ensuring that at the optimum no further improvement is possible without violating constraints.
\end{itemize}

\begin{equation}
    \nabla _{x}\mathcal{L} \ (x,\lambda ,\mu )=\nabla f(x)+\sum _{i=0}^{m} \lambda _{i} \nabla g_{i} (x)+\sum _{j=0}^{p} \mu _{j} \nabla h_ {j} (x)=0
    \label{eq:KKT_Stationarity}
\end{equation}

\begin{itemize}
\item Complementary slackness. The multiplier reflects how much the constraint influences the optimal solution. For each inequality constraint, either the constraint is active and its Lagrange multiplier can be positive, or the constraint is inactive and its multiplier is zero.
\end{itemize}

\begin{equation}
    \lambda _{i} g_{i}(x) =0
    \label{eq:KKT_Comp_Slackness}
\end{equation}

\begin{itemize}
\item Primal feasability. Ensures that the found solution satisfies the original constraints of the problem, which means that the inequality conditions must be held and the equality conditions must be satisfied totally.
\end{itemize}

\begin{equation}
    \begin{aligned}
        & g_{i} (x)\leq 0\\
        & h_{j} (x)=0
    \end{aligned}
    \label{eq:KKT_Primal_Inf}
\end{equation}

\begin{itemize}
\item Dual feasability. Related to inequality constraints multipliers, they verify they are nonnegative values and describe how would the cost and the solution change if these constraints are relaxed or inactivated.
\end{itemize}

\begin{equation}
    \lambda _{i} \geq 0
    \label{eq:KKT_Dual_Inf}
\end{equation}

On its part, the transversality conditions are the boundary conditions for the co-state variables $\lambda$. They are a necessary condition for optimisation in state free-end problems, which means that there's no end known for the trajectory. This transversality conditions can be defined as the existence of a n-vectorial function of the co-state variables $\lambda (t)$ and a m-vectorial function $\nu (t)$ such that the Hamiltonian function $ H(\lambda,\nu,t)$, see equation \eqref{eq:Hamiltonian}, can be defined like in a boundary value problem in his canonical form with \gls{ode}s in all time domain $t_{0} \leq t\leq t_f$ \textcolor{red}{CITAS 11 y 12}.

\begin{equation}
    H\ (\lambda ,\nu ,t)=\lambda ^{T} f+\nu ^{T} g
    \label{eq:Hamiltonian}
\end{equation}

Once the Hamiltonian is defined, both transversality conditions can be found as the partial derivative of the Hamiltonian respect to the state variables and, the evaluation of the co-state at the end of time domain, respectively. Formulation has been adapted from \textcolor{red}{CITAS 11 y 13}.

\begin{equation}
    \dot{\lambda } \ =\ -\frac{\partial H}{\partial y} =-\lambda ^{T}\frac{\partial f}{\partial y} -\nu ^{T}\frac{\partial g}{\partial y}
    \label{eq:Transversality_1}
\end{equation}

\begin{equation}
    \lambda (t_f) =\ \frac{\partial \Phi }{\partial y(t_f)} -\nu ^{T}\frac{\partial \varphi }{\partial y(t_f)}
    \label{eq:Transversality_2}
\end{equation}

Eventually, the \gls{pmp} is a fundamental result of optimal control theory, since it stablishes the necessary conditions to find the optimal trajectory for control variables. In other words, this principle gives the strategy that has to be followed on the control variables, the ones that can be modified through time on the whole problem, to minimise the cost functional while the system dynamic and its constraints are satisfied. From \textcolor{red}{CITA 16}, the \gls{pmp} is defined as the partial derivative of the Hamiltonian function respect the control variables for a problem where $y(0) = y_{0} \in \mathcal{S}$ and $u(t)$, a contol trajectory, has a state trajectory $y(t)$  associated for a given dynamic system. Thus, a co-state trajectory $\lambda (t)$ can be defined as follows.

\begin{equation}
    \begin{aligned}
        & \dot{\lambda } (t)=-\frac{\partial H}{\partial y}\\
        & \lambda (T)=\frac{\partial \Phi }{\partial y(T)}\\
        where & \\
        & H(y,u,\lambda ,t)=L(y,u,t)+\lambda ^{T} f(y,u,t)\\
        & \dot{y} (t)=f(y,u,t)
    \end{aligned}
    \label{eq:PMP_Problem}
\end{equation}

Therefore, the \gls{pmp} can be expressed as the minimisation of the Hamiltonian with respect to the control variables, as indicated in equation \eqref{eq:PMP}. This formulation arises because the original statement of \gls{pmp} is given as a maximisation problem, but it can be equivalently rewritten in minimization form by changing the sign of the Hamiltonian \textcolor{red}{CITA 17} -also called Pontryagin's Minimum Principle-. In essence, the principle reformulates the original optimal control problem into an equivalent system involving the state and an adjoint -or costate- system. The optimal control trajectory is then obtained by ensuring that, at each instant of time, the Hamiltonian is minimised with respect to the admissible controls.

\begin{equation}
    \begin{aligned}
        u(t)=\arg\min_{u\in U} \ H\ (y,u,\lambda ,t)
    \end{aligned}
    \label{eq:PMP}
\end{equation}

\textcolor{red}{PENSAR SI AÑADIR FORMULACIÓN DEL PROBLEMA CON ADJUNTO PARA CLARIFICACIÓN EN APÉNDICES (APÉNDICES A Y B DE LIBRETA)}

\section{Introduction to IPOPT}
The \gls{ipopt} is a robust interior-point line-search algorithm designed to solve large-scale \gls{nlp} problems. This method is particularly well-suited for high-dimensional trajectory optimisation, such as the low-altitude flight manoeuvres required during water-discharge phases in firefighting missions -the primary focus of this thesis-.

\gls{ipopt} identifies local optimal solutions by employing a primal-dual barrier approach to handle inequality constraints and a filter-based line-search method to ensure global convergence. The following subsections detail the mathematical formulation of the \gls{ipopt} algorithm and its practical implementation via the CasADi numerical optimisation framework.

\subsection{Mathematical formulation}

\subsection{Implementation in CasADi}